\documentclass{article}


\title{Hashing and Bloom filters key lookup comparison}
\author{names}
\date{\today}


\usepackage[backend=bibtex]{biblatex}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[a4paper, total={5in, 8in}]{geometry}


\bibliography{references}
\nocite{*}

\begin{document}
    \maketitle
    \thispagestyle{empty}
    \begin{abstract}
	This is the abstract of the document is the abstract of the document is the abstract of the document is the abstract of the document is the abstract of the document
	is the abstract of the document is the abstract of the document is the abstract of the document is the abstract of the document is the abstract of the document
	 is the abstract of the documentis the abstract of the document is the abstract of the document is the abstract of the document is the abstract of the document
	 is the abstract of the document  is the abstract of the document is the abstract of the document is the abstract of the document is the abstract of the document
	 is the abstract of the document is the abstract of the document is the abstract of the document is the abstract of the documentis the abstract of the document
    \end{abstract}
	

    \section{Introduction}
        This project is mainly aimed at learning different implementations of hashing based data structures and their effectiveness in applying a dictionary search problem. \\\\
A dictionary is used to maintain a set under insertion and deletion of elements while membership queries provide access to the data. The most efficient dictionaries, in theory, and practice, are based on hashing techniques. \\\\
To achieve different results and build a complete analysis, we have proposed searching with two types of dictionary data structures: hash tables and bloom filters. \\\\
The main performance parameters that we are going to study, analyze and contrast along this research work are: lookup time, build time, average probes and false positives.
All the algorithms have been implemented in c++ and the same data has been applied to the execution of different versions of the same experiment. 

    \section{Experiment pipeline and methodology}
    	The experimentation process was a very tedious task since it required to take in account a lot of parameters. TODO(Add pipeline image)
    	\subsection{Open Addressing}
Open addressing is one of the two main methods of collision resolution in hash tables. The four main techniques are:

\begin{itemize}
    \item \textbf{Linear probing:} its principle is accomplished using two values: a start index and a stepping value. When a collusion occurs, the table is search sequentially for an empty slot adding repeatedly the second value to the first until either a free space is found or the entire table is traversed.
    \item \textbf{Quadratic probing:} its principle is accomplished using four values: a start index, a stepping value and two constant values. When a collusion occurs, an arbitrary quadratic polynomial value -made by the combination of all the values but the start index- is added to the first one; the finish criteria is the same as the previous technique. The idea is to skip regions in the table with possible clusters.
    \item \textbf{Double hashing:} its principle is accomplished using two values: a start index, a stepping value and a third value. When a collusion occurs, the combination of the stepping and the third value is added to the first one; the finish criteria is the same as the first technique. The idea is to tackle effectively clustering problems.
    \item \textbf{Cuckoo hashing:} its principle is accomplished using two values and either position is computed by the first or the second. When a collusion occurs, the key in the position is replaced and the replaced key is assigned to the position given by the other value. If this new position is occupied, the procedure is repeated until the key is inserted or the iteration reaches an arbitrary value. The replacement behaviour may result in an infinite loop. 
\end{itemize}
\begin{center}
\begin{tabular}{c|c}
    Technique & Function\\
\hline
    LP & h(k,i) = (h(k) + i) mod $m$ \\
\hline
    QP & h(k,i) = (h(k) + $c_1$i + $c_2i^2$) mod $m$ \\
\hline
    DH & h(k,i) = ($h_1$(k) + i*$h_2$(k)) mod $m$ \\
\hline
    QH & h(k) = $h_1$(k) or $h_2$(k) \\
\end{tabular}

\begin{tabular}{c|c}
    Parameters & Meaning \\
\hline
    k & key, value to insert or to search \\
\hline
    i & stepping value \\
\hline
    $c_1$, $c_2$ & constant values \\
\hline
    h(),$h_1$(),$h_2$() & hash functions \\
\end{tabular}
\end{center}

    \subsection{Separate Chaining}
    
    \subsection{Bloom filters}
	Bloom filters \cite{ARTICLE:4} are a space efficient probabilistic data structure that can represent a dictionary. This data structures are space efficient because 
	unlike hash tables they don't store the key itself but the a series of bits that determine if the value is in the dictionary or not. This bits are represented in an array 
	or table. To insert a value into a bloom filter, it has to be hashed with $k$ different hash functions, each hash returns a specific position of the bits table that will be 
	marked with a $1$. Thus, if a value is in the dictionary it means thet if we hash the key to be queried $k$ times every positition will be marked with a $1$ bit. Otherwise 
	if a bit is marked with a $0$ it means that the value is not in the dictionary. This means that we if considered $k$ as a constant we can query and insert always in constant time. 
	On the other hand this causes some problems because it is posible that two keys $k_1$, $k_2$ have hash to the same $k$ bits, and because we don't have a collision mechanism we would introduce 
	a false positive rate when quering a specific value in the dictionary. This rate is directly dependent on the the number of hash functions that we use and the load factor of the table. 
	To minimize \cite{ARTICLE:1} this false positive rate different methods have been found altough you can never make it dissapear. Another problem that we encountered when developing bloom filters 
	was that we needed a way to generate $k$ different hash functions, although we found a very efficient and simple solution \cite{ARTICLE:3} based on creating a new hash function with two initial ones and 
	an iterator. 
	\begin{equation}
	g(Key)_i = h_1(Key) + i * h_2(Key)
	\end{equation}
	
    \section{Data Generation}
        The data used in each of the experiments consisted of a sequence of unsigned integers,  
        the data was written in two different files. One file contained the keys to insert into the dictionary and the other the text to find in the dictionary. 
       
		\subsection*{Linear Congruential Method}
		The randomness of the keys was based on the Linear congruential method (LCM) \cite{BOOK:2}. 
		We needed to have experiments as reliable as possible thus we needed a way to control the data generation, 
		that is why we used this method. The LCM is one of the oldest and best-known random number generators, 
		it is also very easy to implement. This method generates a cyclic sequence of numbers based on an initial seed. 
		Given a seed $X_0$ it is possible to generate the next number in the sequence 
		\begin{equation}
		X_i = (X_{i-1} * a + c) \, \bmod m
		\end{equation}
		 where $a$ is called the multiplier, $c$ the incrementer and $m$ the modulus. 
		 The size of the sequence depends on the parameters used, independently on the initial seed.
		  We used a combination of parameters that yield the maximum size sequence, that is parameters that yield $m$ numbers.  
		  This ensured us that that there where never repeated numbers in either of the two files since the size of $m$ was
		  sufficiently big. 
		  
		   
    \section{Experiment parameters}
    An experiment is purely defined by the subject, represented by the dictionary to test, and the parameters in each the dictionary was specified and tested. 
    \begin{itemize}
    \item \textbf{Size of $n$:}
		   The keys file contained n numbers and the text contained $2 * n + p * n$ where $p$ is the percentage of keys that are already inserted in the dictionary. 
		   We had a lot of discussion in which $n$ used and we tried different combinations but they yielded very similar results so we finally chosed an $n=10e^7$. 
		   That happens to be 10 million keys that where inserted into the dictionary, although the table size was much bigger. We tried to used a bigger size of $n$
		   but we encountered the size of the files we created where extraordinary big, we tried to divide the files but that wasn't of much use since we stilled needed to 
		   save all the numbers in the hash tables. When we tried to to this the linux oom-killer killed to process because of the out of memory that was caused. Nevertheless 
		   other researches \cite{ARTICLE:2} used similar sizes of $n$ so this size wasn't really an issue. 
    \item \textbf{Number of repetitions $Q$:}
    		   Each experiment was defined by a series of parameters to get the best results and avoid overfitting of the data to a single sample we 
    		   repeated each experiment with $Q=100$ that is we performed the same experiment $100$ times and averaged the results. 
   
   \item \textbf{Load factor $\alpha$:}
   		   The load factor is one of the variables that really helps into comparing each type of dictionary implementation, we used in total 
   		   $9$ different load factors ranging from $0.10$ to $0.9$ with an offset of $0.10$. We could have subdivided the interval with a lower 
   		   offset but after experimenting we found that there wasn't really a substantial difference. 
   		   
   \item \textbf{Key percentage $p$:}
   		  The key percentage represents the percentage of keys that are in the keys (inserted in the dictionary) that are in the search file (the second file where the lookups are done). 
   		  This variable did not have a big impact in the results since we average the lokuup time of each key individually and even if we chosed a really low key percentage we had a big enough 
   		  $Q$ that it didn't matter. Thus we chosed $p=0.5$;
   		  
    \item \textbf{Number of hash functions $k$:}
    		 This was a difficult parameter to choose since it had to be chosen optimally because if had a very big k then the number of false positives might diminish, but only if the size of the table
    		 was big enough, on the other hand if we chosed a small value of $k$ the table would fill more slowly but it was more probable to find to keys with the same bits mask. To decide into what to choose 
    		 we simply tested different values of $k$ with different results. 
    		 
    \item \textbf{Hash functions:}
    		The hash functions we chosed where not the purpose of this experiment altough we tried to choose. 
    		
    \item \textbf{Random seed:}
    		The seed number is really a determining factor into how the dictionaries will perform since different seeds yield to different keys and conscuently differnet hash outputs. 
    		That is why for each experiment we chosed the same initial seed, altough the next seed was chosen at random based on the initial seed, this enabled reprodudability. In total 
    		a number of $Q$ different seeds were selected, because the results of the $Q$ repetitions where average this enabled us to remove any outliers that could happend choosing a specific seed value. 
    \end{itemize}
    \section{Results}
    All the results that were obtained are specifically compared to the load factor of the table since it is a determining parameter in the efficiency of each dictionary. 
    
    	\subsection*{Speed performance}
    	To compare the speed...
    	\subsection*{Probes metric}
    	In this section we compare the average probes that were introduced in each search query...
    	\subsection*{False positives}
	    	This section is specifically centred in the false positive rate of the bloom filters...
	    	\begin{figure}[h!]
		  \includegraphics[width=\linewidth]{images/loadFactor_vs_falsePositives.png}
		  \caption{False positive rate comparison}
		  \label{fig:False positive rate}
		\end{figure}
    	
    	
    

    \section{Discussion}
	WE LEARNED THAT....


    \printbibliography

\end{document}
